{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "import pandas as pd\n",
    "\n",
    "# SET OPENAI_API_KEY in .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Prompt + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rater:\n",
    "\n",
    "  'A Rating Class to rate conversations between human and AI'\n",
    "\n",
    "  def __init__(self, compliance_marker=True, helpful_marker=True, relevance_marker=True, efficient_marker=True, reaction_marker=True) -> None:\n",
    "\n",
    "    '''compliance_marker : Compliance to the instructions\n",
    "      \n",
    "    helpful_marker : Degree of helpfulness\n",
    "\n",
    "    relevance_marker : Relevance to the human prompt\n",
    "\n",
    "    efficient_marker : Efficiency towards the answer wanted\n",
    "\n",
    "    reaction_marker : Human response to the answer\n",
    "\n",
    "    All of the markers are factored in at default, if marked False will not be factored in\n",
    "    '''\n",
    "    \n",
    "    # Error incase none of the markers are True\n",
    "    if(not (compliance_marker or helpful_marker or relevance_marker or efficient_marker or reaction_marker)):\n",
    "      raise Exception(\"No Markers Selected for Rating!\")\n",
    "\n",
    "    self.conversations=[]\n",
    "\n",
    "    system = \"You are an unbiased evaluator. Assess the conversations between Human and AI and rate the responses given by AI on a scale on 1 to 10 based on helpfulness, relevance, compliance, and how the human responded\"\n",
    "    human = \"{text}\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "    model = ChatOpenAI()\n",
    "\n",
    "    properties={}\n",
    "    required = []\n",
    "    if compliance_marker:\n",
    "      properties = properties | {\"compliance rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Rate on a scale of 1-10 the response of AI, 1 being least compliant, 10 being most compliant\"},\n",
    "            \"compliance desc\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Justify the rating of compliance\"\n",
    "          }}\n",
    "      required.extend([\"compliance rating\",\"compliance desc\"])\n",
    "    if helpful_marker:\n",
    "      properties = properties | {\"helpful rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Rate on a scale of 1-10 the response of AI, 1 being least helpful, 10 being most helpful\"\n",
    "          },\n",
    "          \"helpful desc\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Justify the rating of helpfulness\"\n",
    "          }}\n",
    "      required.extend([\"helpful rating\",\"helpful desc\"])\n",
    "    if relevance_marker:\n",
    "      properties = properties | {\"relevance rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Rate on a scale of 1-10 the response of AI, 1 being least relevant to the human prompt, 10 being most relevant\"\n",
    "          },\n",
    "          \"relevance desc\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Justify the rating of relevance to the human prompt\"\n",
    "          }}\n",
    "      required.extend([\"relevance rating\",\"relevance desc\"])\n",
    "    if efficient_marker:\n",
    "      properties = properties | {\"efficient rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Rate on a scale of 1-10 how fast and straightforward the AI could get the answers, 1 being inefficient and taking corrections, 10 being very efiicient and direct\"\n",
    "          },\n",
    "          \"efficient desc\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Justify how efficient the AI was in its responses\"\n",
    "          }}\n",
    "      required.extend([\"efficient rating\",\"efficient desc\"])\n",
    "    if reaction_marker:\n",
    "      properties = properties | {\"human-reaction rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Rate on a scale of 1-10 how positively the human responded to the AI response, 1 being very negative, and 10 being very positive\"\n",
    "          },\n",
    "          \"human-reaction desc\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"How well did the human react to the AI response\"\n",
    "          }}\n",
    "      required.extend([\"human-reaction rating\",\"human-reaction desc\"])\n",
    "    functions = [\n",
    "    {\n",
    "      \"name\": \"rating\",\n",
    "      \"description\": \"The assesment of conversation between human and AI\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": properties,\n",
    "        \"required\": required\n",
    "      }\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    # Chain binding the prompt model and output parser to be called for every conversation\n",
    "    self.chain = prompt | model.bind(function_call= {\"name\": \"rating\"}, functions= functions) | JsonOutputFunctionsParser()\n",
    "    \n",
    "\n",
    "\n",
    "  def add_conversation(self, text: str):\n",
    "    'Add a new Conversation to the list'\n",
    "    \n",
    "    self.conversations.append(text)\n",
    "\n",
    "\n",
    "  def get_ratings(self, verbose=True):\n",
    "\n",
    "    '''Get the ratings of all the Conversations added\n",
    "    verbose : get descriptions for the ratings\n",
    "    '''\n",
    "    \n",
    "    self.ratingres=[]\n",
    "    self.totalres =[]\n",
    "    for conversation in self.conversations:\n",
    "      totalrating = self.chain.invoke({\"text\":conversation})\n",
    "      self.ratingres.append({k: v for k,v in totalrating.items() if k.endswith(\"rating\")})\n",
    "      self.totalres.append(totalrating)\n",
    "    if verbose:\n",
    "      return self.totalres\n",
    "    else:\n",
    "      return self.ratingres\n",
    "\n",
    "\n",
    "  def make_csv(self, filename: str):\n",
    "\n",
    "    'Make a CSV file output of all the individual file ratings'\n",
    "    \n",
    "    if len(self.ratingres)==0:\n",
    "      self.get_ratings(verbose=False)\n",
    "    data = {'Conversation':[], 'Compliance':[],'Helpful':[],'Relevance':[],'Efficient':[],'Human-reaction':[]}\n",
    "    data['Conversation']=self.conversations\n",
    "    for rating in self.ratingres:\n",
    "      data['Compliance'].append(rating['compliance rating'])\n",
    "      data['Helpful'].append(rating['helpful rating'])\n",
    "      data['Relevance'].append(rating['relevance rating'])\n",
    "      data['Efficient'].append(rating['efficient rating'])\n",
    "      data['Human-reaction'].append(rating['human-reaction rating'])\n",
    "\n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    data['Aggregate'] = (data['Compliance']+data['Helpful']+data['Relevance']+data['Efficient']+data['Human-reaction'])/5\n",
    "    data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasheet = pd.read_csv(\"conversations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "PASTE SAMPLE CONVERSATION HERE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'compliance rating': 10, 'helpful rating': 10, 'relevance rating': 10, 'efficient rating': 8, 'human-reaction rating': 10}, {'compliance rating': 9, 'helpful rating': 8, 'relevance rating': 10, 'efficient rating': 8, 'human-reaction rating': 8}, {'compliance rating': 9, 'helpful rating': 8, 'relevance rating': 9, 'efficient rating': 8, 'human-reaction rating': 10}, {'compliance rating': 9, 'helpful rating': 9, 'relevance rating': 9, 'efficient rating': 8, 'human-reaction rating': 8}, {'compliance rating': 9, 'helpful rating': 8, 'relevance rating': 9, 'efficient rating': 7, 'human-reaction rating': 7}, {'compliance rating': 9, 'helpful rating': 9, 'relevance rating': 10, 'efficient rating': 10, 'human-reaction rating': 8}, {'compliance rating': 9, 'helpful rating': 9, 'relevance rating': 10, 'efficient rating': 8, 'human-reaction rating': 9}]\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Rater\n",
    "Chatrater = Rater()\n",
    "\n",
    "# Loading the conversations into the Rater class\n",
    "for idx in datasheet.index:\n",
    "  Chatrater.add_conversation(datasheet['Conversations'][idx])\n",
    "\n",
    "# UNCOMMENT IF USING A SINGLE TEXT\n",
    "# Chatrater.add_conversation(text)\n",
    "\n",
    "# Getting the Ratings of each conversation\n",
    "print(Chatrater.get_ratings(verbose=False))\n",
    "\n",
    "# Making a csv file to store the ratings\n",
    "Chatrater.make_csv('conversation2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "# Defining the criterias for scoring - Helpful, Relevance, Efficient, Reaction, Compliance\n",
    "\n",
    "criteria = {\n",
    "  \"helpful\": \n",
    "    \"\"\"Score 1: The answer is unhelpful and provides no assistance.\n",
    "        Score 3: The answer offers limited help but lacks depth or detail.\n",
    "        Score 5: The answer is moderately helpful, providing relevant information.\n",
    "        Score 7: The answer is quite helpful, offering comprehensive information.\n",
    "        Score 10: The answer is extremely helpful, addressing all needs effectively.\"\"\",\n",
    "    \"relevance\": \n",
    "    \"\"\"Score 1: The answer is entirely irrelevant to the human prompt.\n",
    "        Score 3: The answer is loosely related but misses the main point.\n",
    "        Score 5: The answer is somewhat relevant but may have minor tangents.\n",
    "        Score 7: The answer is highly relevant, closely tied to the human prompt.\n",
    "        Score 10: The answer is perfectly relevant, directly addressing the prompt.\"\"\",\n",
    "    \"efficient\": \"\"\"\n",
    "        Score 1: The answer is inefficient and fails to provide the desired information.\n",
    "        Score 3: The answer is somewhat efficient but lacks clarity or conciseness.\n",
    "        Score 5: The answer is reasonably efficient, delivering the needed information.\n",
    "        Score 7: The answer is quite efficient, presenting information clearly.\n",
    "        Score 10: The answer is extremely efficient, delivering a concise response.\"\"\",\n",
    "    \"reaction\": \"\"\"\n",
    "        Score 1: The human response to the answer is negative or frustrated.\n",
    "        Score 3: The human response is neutral or somewhat dissatisfied.\n",
    "        Score 5: The human response is generally positive but not enthusiastic.\n",
    "        Score 7: The human response is positive and satisfied with the answer.\n",
    "        Score 10: The human response is highly positive and appreciative of the answer.\"\"\",\n",
    "    \"compliance\": \"\"\"\n",
    "        Score 1: The answer completely misses the user's instructions.\n",
    "        Score 3: The answer partially addresses the user's instructions but lacks depth.\n",
    "        Score 5: The answer aligns with the user's prompt but may have minor gaps.\n",
    "        Score 7: The answer closely aligns with the user's prompt and is comprehensive.\n",
    "        Score 10: The answer perfectly fulfills the user's intent with a flawless answer.\"\"\"\n",
    "}\n",
    "\n",
    "# Loading the evaluator with the criterias\n",
    "evaluator = load_evaluator(\n",
    "    \"score_string\", \n",
    "    criteria=criteria, \n",
    "    llm=ChatOpenAI(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"USER\\nhow do you write a formal email in 50 words\\nASSISTANT\\nSubject: Formal Email Request\\n\\nDear [Recipient's Name],\\n\\nI hope this email finds you well. I am writing to request your guidance regarding [specific issue]. Your expertise and insight would be greatly appreciated. Kindly let me know your availability to discuss this matter further.\\n\\nThank you for your attention to this matter.\\n\\nSincerely,\\n[Your Name]\": {'reasoning': 'This response is quite helpful as it provides a clear structure and format for writing a formal email in a concise manner. It offers relevant information by addressing the main components of a formal email, such as the subject, salutation, body, and closing. The response is also efficient, delivering the necessary information in a concise manner. It is likely to receive a positive reaction from the user as it fulfills the instructions by providing a formal email example within the requested word limit.\\n\\nRating: [[8]]',\n",
       "  'score': 8},\n",
       " '\\ni dont know the recipients name\\nASSISTANT\\nSubject: Formal Email Request\\n\\nDear Sir/Madam,\\n\\nI hope this email finds you well. I am writing to request your guidance regarding [specific issue]. Your expertise and insight would be greatly appreciated. Kindly let me know your availability to discuss this matter further.\\n\\nThank you for your attention to this matter.\\n\\nSincerely,\\n[Your Name]': {'reasoning': \"The assistant's response is not relevant to the user's question. The user asked about not knowing the recipient's name, but the assistant's response is a template for a formal email request. Although the response is somewhat efficient and the compliance is partially met (as it does provide a template for a formal email request), it is not helpful or relevant to the user's specific situation. \\n\\nRating: [[3]]\",\n",
       "  'score': 3},\n",
       " '\\ni dont wanna put my name\\nASSISTANT\\nSubject: Formal Email Request\\n\\nDear Sir/Madam,\\n\\nI hope this email finds you well. I am writing to request your guidance regarding [specific issue]. Your expertise and insight would be greatly appreciated. Kindly let me know your availability to discuss this matter further.\\n\\nThank you for your attention to this matter.\\n\\nSincerely.': {'reasoning': \"The AI assistant's response is not relevant to the user's question. The user asked for assistance with not wanting to put their name, but the assistant provided a template for a formal email request. The response is also not efficient as it does not address the user's actual concern. The compliance is low as the assistant completely missed the user's instructions. The response is unhelpful and irrelevant.\\n\\nRating: [[1]]\",\n",
       "  'score': 1},\n",
       " '\\nchange the wording a bit\\nASSISTANT\\nSubject: Formal Inquiry\\n\\nDear Sir/Madam,\\n\\nI trust this email finds you in good health. I am writing to respectfully request your expertise on [specific issue]. Your insights and guidance would be highly valuable. Please let me know when you would be available for a discussion.\\n\\nThank you for your kind attention.\\n\\nSincerely.': {'reasoning': \"This response does not address the user's question at all. The user asked for a change in wording, but the assistant provided a generic email template instead. The response is unhelpful, irrelevant, and does not comply with the user's instructions.\\n\\nRating: [[1]]\",\n",
       "  'score': 1},\n",
       " \"\\nI love it\\nASSISTANT\\nI'm glad you liked it! If you have any further questions or need assistance with anything else, feel free to ask.\\n\": {'reasoning': \"This response from the AI assistant is relevant and compliant with the user's prompt. It acknowledges the user's statement and offers further assistance if needed. However, it lacks depth and detail, providing limited help to the user. The response is also efficient and does not require any additional information. There is no reaction marker provided by the user. \\n\\nRating: [[5]]\",\n",
       "  'score': 5},\n",
       " 'final_score': 3.6}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = datasheet['Conversations'][2]\n",
    "scores = 0\n",
    "reply_pairs = conversation.split(\"\\nUSER\")\n",
    "rating = {}\n",
    "\n",
    "# Splitting the conversation into Human - AI reply pairs and aggregrating the results\n",
    "for reply_pair in reply_pairs:\n",
    "  reply_pair_split = reply_pair.replace(\"USER\", \"\").split(\"ASSISTANT\")\n",
    "  try:\n",
    "    evaluate =evaluator.evaluate_strings(\n",
    "      prediction=reply_pair_split[1],\n",
    "      input=reply_pair_split[0])\n",
    "    rating[reply_pair]=evaluate\n",
    "    scores += evaluate['score']\n",
    "  \n",
    "  # *** LOOK AT *** GIVING ERROR IF LLM MARKS RESPONSE AS 10\n",
    "  except ValueError as e:\n",
    "    print(e)\n",
    "rating['final_score']=scores/len(reply_pairs)\n",
    "\n",
    "# storing all reply pairs and their ratings as well as the aggregated final score into rating\n",
    "rating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
